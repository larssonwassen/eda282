\documentclass[a4paper]{article}

\renewcommand{\labelenumi}{(\alph{enumi})}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{fancyhdr}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\renewcommand{\labelenumi}{(\alph{enumi})}
\pagestyle{fancy}

\lhead{Chalmers - EDA282}
\chead{Excercise 2}
\rhead{Dan Larsson}
\lfoot{Oktober 2014}


\begin{document}
\section*{}
I have discussed my solutions with Jonas Hemlin.
%\textcolor{red}{\huge Changed stuff marked in red.}

\section*{Q1}
\large{\textbf{What is Amdahl’s law? (give the equation, but then explain in [your own] words) 2 points\\}}
$S=\frac{1}{B + \frac{1}{n}\times(1-B)}$, $S$ is the speedup, $n$ the number of threads, $B$ the fraction of the program performed serially. Amdahl’s law is used to calculate the speedup gained from parallelization of a task, compared to the serial version. It can also be used for calculating the total speedup on a system from improving a subsystem.

\section*{Q2}
\large{\textbf{What is perfect scaling? Why do we fail to achieve perfect scaling by doubling (a) the number of transistors, (b) the number of functional units, (c) the number of cores, or (d) the clock speed? Give a sentence or two of explanation for each of (a)-(d). 5 points\\}}
Amdahl's law can be used for all these cases and it would show the actual speedup gained.
\begin{enumerate}
\item
One have to know what to do with all the new transistors. Simply adding more lines to the caches or widening the memory bus will double the performance as all parts in the system have to be improved on. 
\item
The system will not be able to utilize all FU at the same time without also beefing up the units feeding them with data. 
\item
As the code that can be distributed for parallel execution in a program is not 100\%, the gain from increasing the number of cores will diminish.  
\item
When increasing just the clock speed, long latency operations like cache misses and I/O does not gain any speed improvements and thereby holds the system back.
\end{enumerate}

\section*{Q3}
\large{\textbf{
What is a CHARM architecture? What two coordination mechanisms are needed in CHARM architectures? 5 points\\}}
CHARM stands for Coordinated Homogeneous Arrays of RISC Microprocessors and is a collection, ranging from a few to thousands, of simple processors working in parallel. They communicate by fast networks which scales well with the number of processors.  The two coordination mechanisms needed in a CHARM architecture is, a low latency interconnect allowing processors to access other processors memories and data, and a mechanism for keeping global state between the processors in an efficient way. 

\section*{Q4}
\large{\textbf{
What family of processor architectures has been used in both data parallel and thread parallel machines? (not the same version) 2 points\\}}
The CM-5 processor architecture is thread parallel and data parallel, while the previous CM machines only is data parallel. Both belongs to the Connection Machine processor family by TMC.

\section*{Q5}
\large{\textbf{
Why was the cache ineffective in the CM-5? 2 points\\}}
Small RISC processors depend on caches to bridge the speed gap of the memory and the processor, but in most of the applications running on a CM-5 almost all of main memory is accessed during one program iteration meaning that the  data in the cache will be written over and the hit rate of the cache will be low. 

\section*{Q6}
\large{\textbf{
How does simultaneous multithreading (SMT) make a single core appear to the software to be multiple cores running multiple threads? 5 points\\}}
The processor keeps state for several threads and can dispatch instructions for the different threads interleaved. When a thread starts a long latency operation, the other threads can execute and make progress. The threads are seen as processors on their own by the OS, meaning that it simply schedules processes for them as it would in as system with many actual processors or singled threaded cores.

\section*{Q7}
\large{\textbf{
In an architecture that supports SMT, some resources are shared completely, some are partitioned to limit each thread’s usage, and some are duplicated to provide private copies. Indicate S, P, or D and include an explanation (e.g., why or how). For shared and partitioned resources, say how many logical threads can share them. (23 points -- 1 point for each S/P/D plus explanation) for the \\}}

\textbf{Intel Pentium:} two Hyperthreads.
\begin{enumerate}
\item    
\textbf{trace cache (the special instruction cache holding sequences of decoded uops)\\}
S - Shared by the logical processors. Stores decoded instructions.
\item
\textbf{ucode ROM\\}
S - Shared by the threads (logical processors). Used to store complex IA-32 instructions.
\item
\textbf{instruction TLB (iTLB)\\}
D - Translates instructions addresses to physical addresses on TC misses.
\item
\textbf{instruction fetch unit\\}
S - Shared by the threads (logical processors).
\item
\textbf{instruction decoder logic\\}
S and P - The regular uop decoder is shared while the IA-32 decoder is partitioned.
\item
\textbf{instruction queues (memory op queue and “all other” op queue)\\}
P - Made sure so that not one thread occupy all entries except for when running self.
\item
\textbf{retirement logic\\}
S - Shared by the threads (logical processors).
\item
\textbf{uop schedulers\\}
S - Shared by the threads (logical processors). The op buffer is however partitioned so one thread can not occupy all places.
\item
\textbf{general purpose architected registers\\}
D - Each thread (logical processor) can utilize all architecture registers.
\item
\textbf{Register Alias Table (RAT) (used for expanding the few architected registers to which the ISA refers, mapping them onto a much larger physical register file)\\}
D - Each thread (logical processor) keeps track of the complete architectural state.    
\item
\textbf{physical registers\\}
S - Register renaming makes sure each processor gets it’s own register.
\item
\textbf{branch predictor’s Return Stack Buffer (RBS)\\}
D - Better predictions if done per thread.
\item
\textbf{branch predictor’s global history array\\}
S - Entries tagged with thread (logical processor) id.
\item
\textbf{data cache (shared)\\}
S - Shared by the threads (logical processors).
\item
\textbf{interrupt controller\\}
D - for each thread(logical processor) as it handles it’s own interrupts.

\textbf{Sun rock:} Two threads (16 cores, 32 threads per chip).  
\item
\textbf{instruction fetch unit\\}
P - The IFU is shared among four cores, which also means that all threads in those cores share the IFU to (8 threads). The IFU fetches instruction in a round robin fashion meaning that no one thread can use all resources.

\item
\textbf{instruction decoder logic\\}
P - One Decoder in each pipeline, which there are one per core (2 threads) and the instruction is decoded in a round robin fashion. 

\item
\textbf{instruction TLB (iTLB)\\}
S - There is one iTLB per iCache, and there is one iCache in each IFU, meaning four cores(8 threads) share one iTLB.

\item
\textbf{retirement logic\\}
S - There is retirement logic in pipeline, which there are one for each core (2 threads).

\item
\textbf{data cache\\}
S - One D-Cache is shared between two cores (4 threads), but each core have it's own tag array(2 threads).

\item
\textbf{FPU\\}
S - One FPU is shared between two cores (4 threads).

\textbf{Sun Niagara:} Four threads per Sparc Pipe (eight Sparc Pipes per chip, 32 threads).
\item
\textbf{data TLB\\}
S - Each Sparc Pipe have it's own DTLB. (4 threads)

\item
\textbf{store buffers\\}
D - Each thread have its own store buffers. The buffers is used to check for RAW hazards.

\end{enumerate}
\section*{Q8}
\large{\textbf{
How many threads can run on one physical Intel Pentium core? On one Sun Rock core? On one Niagara Sparc pipe? 3 points\\}}
The Intel Pentium can run two threads, Sun Rock two threads, and Sparc Niagara four threads per Sparc Pipe.

\section*{Q9}
\large{\textbf{
Name the three processors we’ve talked about (in papers or in lecture slides) that support Transactional Memory in hardware. Which ones are available in commercial products? 3 points\\}}
The Intel's Haswell uses HTM and is commercially available. Sun's Rock processor include HTM and was commercially available but got canceled. BlueGeneQ made by IBM is part of Top 500 supercomputers, thus commercial, and uses HTM.

\section*{Q10}
\large{\textbf{
Name one advantage and one disadvantage of having SIMD ISA extension instructions and FP instructions share resources. Explain your answers. Which specific resources are shared in the Intel Pentium/MMX architecture? In the Sun Rock/Vis architecture? 5 points\\}}
One advantage is that there is no need for adding additional hardware for keeping the architectural state and thus simplifies the design and saves hardware, while as the drawback is that you can not us FP instructions in the same routine as the added instructions without corruption of the data occurring. For the MMX architecture, the added registers maps onto the FP registers, meaning these are shared between the two. It is the registers that are shared in Sun Vis to.

\section*{Q11}
\large{\textbf{
What is a data race? What is atomicity? What is sequential consistency? 3 points\\}}
Data race = When two or more threads accesses and writes to the same address without proper synchronization a data race arises.
atomicity = A operation is performed mutually exclusive, that is, the operation is performed without any disturbance.  
SC = Sequential Consistency is enforced if the end result of a parallel execution becomes the same as if the operations had been performed sequentially. 

\section*{Q12}
\large{\textbf{
Assume x is initially 999. Explain how, when executed by two threads simultaneously, the following code can result in x holding either 1 or 2000. 10 points\\}}
\textit{x is number variable represented in a way that requires two words on the target machine. Assume x is stored in two parts, x\_hi and x\_lo, each holding three decimal digits. you can view highword() and lowword() as \#define’s in C (syntactic sugar) to access the hi and lo parts of a number variable incr() does whatever is required to add one to its number variable argument.}
\lstset{language=C,frame=single}
\begin{lstlisting}
reg1 = highword(x);
reg2 = lowword(x);
x = incr(x);
highword(x) = reg1;
lowword(x) = reg2; 
\end{lstlisting}
The first case, x becomes 1, happens when one thread first reads the high word equal to 0, and then gets context switched. The other thread then completes it's code and increases x to 1000. When the first thread then executes again it reads the low word, now equal to 0 instead of 999, increases it to 1, and writes it back.

For the second case, almost the same thing happens, the first thread executes until it has stored the high word of x increased to 1, gets context switched, and the other thread reads 1999 instead of 999. Thread two then increments 1999 to 2000, gets switched out for the first thread that stores 1000, gets switched back and stores 2000.  

\section*{Q13}
\large{\textbf{
Explain the difference between write invalidate and write update cache coherence protocols. 5 points\\}}
When using the Write-Invalidate CCP, a memory write to a shared address will cause invalidations to be sent to all caches holding that data. When using Write Update CCP, instead of invalidating the value in all the other caches, the new value will be distributed to update their values.

\section*{Q14}
\large{\textbf{
What are the possible cache line states in the Blue Gene/P coherence protocol? 2 points\\}}
The blue Gene/P uses write-through for both L1 and L2 and does not have any hardware CCP the cache lines can be Valid or Invalid (Two states).

\section*{Q15}
\large{\textbf{
How does a snoop filter improve performance and save energy? (be specific about which coherence commands it filters when) 5 points\\}}
A snoop filter removes invalidates and snoops for a cache line that is not in the cache, thus removing unnecessary snoops and invalidates. The improvements gained by a snoop filter depends on both the type of filter used and of the cache it is used with. For a destination based snoop filter there is small energy savings and if the cache is multi-ported, snoops and invalidates does not have to compete with regular accesses, so the performance gain is also minimal. This as the snoops and invalidates still traverse the bus draining energy and causing congestion and the added hardware for the filter consumes energy. For a source based snoop filter the commands will not need to traverse the bus and will thus save more energy and cause less congestion (performance up).

\section*{Q16}
\large{\textbf{
What limitation of the BG/P L1 cache makes the snoop filter critical to performance? 2 points\\}}
The L1 cache in the BG/P only have one access port meaning that snoops and invalidates would compete with regular accesses. THis together with the caches being write-through and invalidations being sent to all caches every time a value is written makes the snoop filters very important.

\section*{Q17}
\large{\textbf{
Could a snoop filter be used to improve performance/efficiency of a bus-based write-update protocol? How, or why not? 2 points\\}}
With a source based snoop-filter, snoops for data that does not exist in a cache is removed and never put on the bus. This will therefore lower the traffic on the bus and thus increase performance. Also, in a single access-port memory the snoops would compete with the normal accesses and removing unnecessary snoops would therefore decrease congestion of the port and increase performance.

\section*{Q18}
\large{\textbf{
What is dimension-order routing? When/why is it used? 2 points\\}}
Dimension order routing is when a message is routed through a system one dimension at a time. If node N(0,0) sends a packet to N(10,7) the packet will go to node N(10,0) and then continue on to N(10,7). The same applies in a cubic interconnect, but simply with one more dimension.  

\section*{Q19}
\large{\textbf{
What is a flit (in general, not in the context of any specific network)? Is it different from a packet? Explain. 2 points\\}}
Flow control unit, flit, is the part of a packet that gets blocked in the case of congestion. In a cut-through switching a flit is the entire packet.

\section*{Q20}
\large{\textbf{What network topology was used in the Connection Machine CM-5? Why? 5 points\\}}
The data network topology of the CM-5 is Fat tree which is used as it scales well with the number of nodes. For sending packets around the network a packet-switched router is used and adaptive load balancing that together minimizes the network congestion and makes it deadlock-free. A fat tree's bandwidth naturally increases with the number of nopes which makes it perfect for a scalable application. It also have good fault tolerant features.

\section*{Q21}
\large{\textbf{
In the Stanford DASH system, bus bandwidth imposed a greater limit on remote memory bandwidth than the network bisection bandwidth. Explain a) what this means (e.g., what’s the bisection bandwidth?), and b) why was this the case? 5 points \\}}
Bisection bandwidth is the bandwidth through the path in a interconnect split into two parts at most narrow point. E.g. in a tree interconnect the bisection bandwidth would be 1u as we would cut the tree at the root were only one vertex exist. In DASH each nodes memory bus is connected to a global bus, and when a remote request is issued the local bus will be traversed twice (local-$>$global-$>$local) and thus the traffic on the local bus will be twice as high. This means that the local bandwidth will be more important than the bisection bandwidth.   

\section*{Q22}
\large{\textbf{In what types of application areas are VLIW processors employed today? 2 points\\}}
In processing of digital signals (DSPs) and multimedia.


\end{document}















