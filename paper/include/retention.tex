\section{Retention}
\label{sec:ret}

Zehan Cui et al. \cite{dtail} propses DTail-R in which data for line refresh rate is stored in the DRAM itself instead of storing it in the memory controller which many other solutions do \cite{raidr}\cite{smartrefresh}\cite{refrint}. To counter the added latency of a slower memory they utilizes spatial locality of the refresh data and performs prefetching into a FIFO buffer. The retention time data consists of 4 bits and is interpreted in such way that the refresh period becomes \(64ms \times 2^n, n = [0...7]\). With this structure the amount of memory required is weighing in on a negliable amount of 0.006\% DRAM capacity (i.e. 1.92MB in a 32GB DRAM). One thing that DTail-R misses is refreshrate compensation for temperature variations. As the refresh rate is set per line DTail-R gets a higher resolution than techniques that set refreshrate on a higher level. Therefore DTail-R manages to decrease the amount of refreshes with as much as 87.9\% and thereby only have 16.7\% energy overhead compared to a refresh less 32GB DRAM. Zehan Cui et al. also proposes DTail-V \cite{dtail} which can be combined with DTail-R and is further discussed in \ref{sec:val}.

RAIDR by Jinghui Liu et al. \cite{raidr} divides all memory lines into different groups, called bins, based on the cell with the shortest retention time in the line. In the basic configuration there are two bins, one with a refreshrate of \(64ms - 128ms\) and one with \(128ms - 256ms\). Lines in the first bin gets refreshed every \(64ms\), lines in the second bin every \(128ms\), and lines not in any bin gets refreshed at the default refresh rate which is set to \(256ms\). The implementation is done inside the memory controller and to minimize the storage needed for keeping track of bin members they use Bloom filters. Thanks to this only 1.25 KB storage is needed for a 32 GB DRAM with the default configuration. The storage requirement can however vary depending the amount of bins used. RAIDR also acomodate for temperature variations by scaling all bins refresh rates using a timer. With RAIDR a performance improvment of 4.1\% and a decreas in powerconsumption by 8.3\% is achieved in a system with 32GB DRAM compared to auto-refresh.
  
RIO is a technique proposed by Seungjae Baek et al. \cite{rioparis} in which they identify cells with low retention time (bad cells) and `deletes' them. This is done by changing the memory allocating modules in the OS to not allocate pages that have bad cells in them. They implement RIO in a Linux kernel which is run and tested on a real hardware system and is able to achieve 87.5\% reduction in refresh count with a performance increas of 4.5\% on average. As the implementaion only relies on changes in the kernel and there is no need to modify the hardware this technique would be easy to adopt in systems today. To keep fragmentation of the memory low a maximum of 0.1\% of total memory pages gets `deleted'. Even though deletion of a larger amount of pages could decrease the amount of refreshes further it would cause trouble for the kernel which relies on being able to allocate large contiguous address spaces. BLASDLBASDAHSUDGASJHDGJHASGDJHASGDJHAGSDJHGASJHDGAJSHDand it is temperature aware. Seungjae Baek et al. also propose PARIS discussed in \ref{sec:val} which can be combined with RIO for further improvments.

With SECRET Chung-Hsiang Lin et al. \cite{secret} masks bad cells using Error Correction Pointers (ECP). The bad cells are maped with a one time profiling done before first system use and all cells that dont meet the retention time requirements gets a ECP. A ECP is basiclly a pointer to another cell that is healthy. The framwork which is implemented inside the memory controller checks whether there is any valid ECP for every address accessed. If there is, it replaces the bad cell(s) data with the data from ECP cell(s) instead.
This method gives a high energy saving of up to 87.2\% in refresh power and a reduction of 18.57\% in total DRAM power consumption with the RCP cach overhead taken into account.
The performance impact ranges from 1.3\% overhad att the worst test case to 1.4\% improvment at its best. The degredation of performance is due to the added memory access needed and the improvment is for the decreased congestion due to refreshes.
